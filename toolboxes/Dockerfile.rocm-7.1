# build stage
FROM registry.fedoraproject.org/fedora:43 AS builder
# Toggle llama.cpp PR 15405 (RPC perf for large models)
ARG APPLY_LLAMA_PR_15405=1

# rocm 7.1 repo
RUN <<'EOF'
tee /etc/yum.repos.d/rocm.repo <<REPO
[ROCm-7.1]
name=ROCm7.1
baseurl=https://repo.radeon.com/rocm/el9/7.1/main
enabled=1
priority=50
gpgcheck=1
gpgkey=https://repo.radeon.com/rocm/rocm.gpg.key
REPO
EOF

# deps
RUN dnf -y \
    install \
      make gcc cmake lld clang clang-devel compiler-rt libcurl-devel ninja-build \
      ca-certificates libatomic libstdc++ libgcc libgomp \
      rocm-llvm rocm-device-libs hip-runtime-amd hip-devel \
      rocblas rocblas-devel hipblas hipblas-devel rocm-cmake libomp-devel libomp \
      rocminfo radeontop \
      git-core vim sudo rsync \
      calibre ffmpeg nodejs mecab espeak-ng rust sox git \
      micro bat uv \
 && dnf clean all && rm -rf /var/cache/dnf/*

# rocm env
ENV ROCM_PATH=/opt/rocm \
    HIP_PATH=/opt/rocm \
    HIP_CLANG_PATH=/opt/rocm/llvm/bin \
    HIP_DEVICE_LIB_PATH=/opt/rocm/amdgcn/bitcode \
    PATH=/opt/rocm/bin:/opt/rocm/llvm/bin:$PATH \
    ESPEAK_LIBRARY=/usr/lib64/libespeak-ng.so.1 \
    MIOPEN_FIND_MODE=FAST \
    MIOPEN_USER_DB_PATH="~/tts/miopen_cache"

# llama.cpp
WORKDIR /opt/llama.cpp
RUN git clone --recursive https://github.com/ggerganov/llama.cpp.git .

# build
RUN git clean -xdf \
 && if [ "${APPLY_LLAMA_PR_15405}" = "1" ]; then \
      git config user.email "builder@localhost"; \
      git config user.name "Container Builder"; \
      git fetch origin pull/15405/head:pr-15405; \
      git merge --no-edit pr-15405; \
    fi \
 && git submodule update --recursive \
 && cmake -S . -B build \
      -DGGML_HIP=ON \
      -DAMDGPU_TARGETS=gfx1151 \
      -DCMAKE_BUILD_TYPE=Release \
      -DGGML_RPC=ON \
      -DLLAMA_HIP_UMA=ON \
      -DGGML_CUDA_ENABLE_UNIFIED_MEMORY=ON \
      -DROCM_PATH=/opt/rocm \
      -DHIP_PATH=/opt/rocm \
      -DHIP_PLATFORM=amd \
      -DCMAKE_HIP_FLAGS="--rocm-path=/opt/rocm" \
 && cmake --build build --config Release -- -j$(nproc) \
 && cmake --install build --config Release

# libs
RUN find /opt/llama.cpp/build -type f -name 'lib*.so*' -exec cp {} /usr/lib64/ \; \
 && ldconfig

# helper
COPY gguf-vram-estimator.py /usr/local/bin/gguf-vram-estimator.py
RUN chmod +x /usr/local/bin/gguf-vram-estimator.py

# copy
RUN cp /opt/llama.cpp/build/bin/rpc-* /usr/local/bin/

# ld
RUN echo "/usr/local/lib"  > /etc/ld.so.conf.d/local.conf \
 && echo "/usr/local/lib64" >> /etc/ld.so.conf.d/local.conf \
 && ldconfig \
 && cp -n /usr/local/lib/libllama*.so* /usr/lib64/ 2>/dev/null || true \
 && ldconfig

# profile
RUN printf '%s\n' \
  'export ROCBLAS_USE_HIPBLASLT=1' \
  'export ESPEAK_LIBRARY=/usr/lib64/libespeak-ng.so.1' \
  'export MIOPEN_FIND_MODE=FAST' \
  'export MIOPEN_USER_DB_PATH="~/tts/miopen_cache"' \
  > /etc/profile.d/rocm.sh && chmod +x /etc/profile.d/rocm.sh \
  && echo 'source /etc/profile.d/rocm.sh' >> /etc/bashrc

# code server
RUN curl -fsSL https://code-server.dev/install.sh | sh

# pm2
RUN npm install pm2 -g

# shell
CMD ["/bin/bash"]
